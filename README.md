# Game AI: Minimax vs Q-Learning

This repository implements and compares **search-based** (Minimax, Minimax with Alpha-Beta Pruning) and **learning-based** (Q-Learning) AI strategies for two classic board games: **Tic Tac Toe** and **Connect 4**.

---

## ðŸ“– Project Overview
The project investigates algorithm performance under different game complexities:

- **Tic Tac Toe**: Small state space, solvable exhaustively.  
- **Connect 4**: Large state space, requiring depth limits and heuristics.

### ðŸ”‘ Key Takeaways
- **Minimax** guarantees optimal play but is computationally expensive.  
- **Alpha-Beta Pruning** reduces node exploration by up to **87%**.  
- **Q-Learning** underperforms in Tic Tac Toe but achieves **54% win rate** in Connect 4.  

---

## ðŸ“‚ Folder Structure

```
â”œâ”€â”€ algorithms/        # Minimax, Minimax-AB, Q-Learning implementations
â”œâ”€â”€ experiments/       # Scripts for running and benchmarking experiments
â”œâ”€â”€ games/             # Game logic for Tic Tac Toe and Connect 4
â”œâ”€â”€ models/            # Saved Q-learning models / state representations
â”œâ”€â”€ opponents/         # Semi-intelligent baseline opponents
â”œâ”€â”€ results/           # CSVs, logs, and evaluation outputs
â”œâ”€â”€ utils/             # Helper functions (evaluation, metrics, configs)
â”œâ”€â”€ visualization/     # Plotting and result visualization tools
â”œâ”€â”€ demo.py            # Example run showcasing algorithms in action
â”œâ”€â”€ main.py            # Main entry point to run experiments
â””â”€â”€ README.md
```

---

## ðŸš€ Getting Started

### Prerequisites
- Python 3.8+  
- Recommended: create a virtual environment  
- Install dependencies:
```bash
pip install -r requirements.txt
```

### Run Experiments
Run demo games:
```bash
python demo.py
```

Run full experiment suite:
```bash
python main.py
```

Results will be saved in the `results/` folder.  

---

## ðŸ“Š Example Results

- **Tic Tac Toe**  
  - Minimax & Minimax-AB dominate with >80% win rate vs baseline.  
  - Q-Learning struggles, achieving only ~8%.  

- **Connect 4**  
  - Minimax (depth=4) achieves ~100% win rate.  
  - Q-Learning is more competitive, ~54% win rate.  

## ðŸ“ˆ Visualization

**Algorithm Performance Summary**

| Game        | Algorithm       | Win Rate (%) | Draw Rate (%) | Loss Rate (%) |
|-------------|-----------------|--------------|---------------|---------------|
| Tic Tac Toe | Minimax         | 84.0         | 16.0          | 0.0           |
| Tic Tac Toe | Minimax-AB      | 87.0         | 13.0          | 0.0           |
| Tic Tac Toe | Q-Learning      | 8.0          | 24.0          | 68.0          |
| Connect 4   | Minimax         | 100.0        | 0.0           | 0.0           |
| Connect 4   | Minimax-AB      | 96.0         | 2.0           | 2.0           |
| Connect 4   | Q-Learning      | 54.0         | 18.0          | 28.0          |

---

## References
- Allis, V. (1988). *A knowledge-based approach of Connect-4*.  
- Sutton & Barto (2018). *Reinforcement Learning: An Introduction*.  
- Campbell et al. (2002). *Deep Blue*.  

---

## Author
**Vishaalini Ramasamy Manikandan**  

